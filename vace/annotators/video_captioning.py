# /data/VACE/vace/annotators/video_captioning.py
# -*- coding: utf-8 -*-
# Copyright (c) Alibaba, Inc. and its affiliates.

import cv2
import os
import logging
import base64
import requests
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import tempfile
from typing import Optional, Union
import numpy as np

class VideoCaptioning:
    """
    ë‹¨ì¼ ì¥ë©´ ë¹„ë””ì˜¤ ìë™ ìº¡ì…”ë‹ í´ë˜ìŠ¤
    ì¤‘ê°„ í”„ë ˆì„ ì¶”ì¶œ + ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë°©ì‹
    """
    
    def __init__(self, method="blip2", device="auto"):
        """
        Args:
            method: ìº¡ì…”ë‹ ë°©ë²• ("blip2", "gpt4v")
            device: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©ì‹œ ë””ë°”ì´ìŠ¤ ("auto", "cuda", "cpu")
        """
        self.method = method
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == "auto":
            if torch.cuda.is_available():
                self.device = f"cuda:{torch.cuda.current_device()}"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.processor = None
        self.model = None
        
        if method == "blip2":
            self._init_blip2()
        
        logging.info(f"VideoCaptioning initialized - method: {method}, device: {self.device}")
    
    def _init_blip2(self):
        """BLIP-2 ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logging.info("Loading BLIP-2 model...")
            self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
            self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì´ë™
            if "cuda" in self.device:
                self.model = self.model.to(self.device)
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ half precision ì‚¬ìš©
                if torch.cuda.is_available():
                    self.model = self.model.half()
            
            self.model.eval()  # í‰ê°€ ëª¨ë“œ
            logging.info("BLIP-2 model loaded successfully")
            
        except Exception as e:
            logging.error(f"Failed to load BLIP-2 model: {e}")
            self.processor = None
            self.model = None
            raise
    
    def extract_middle_frame(self, video_path: str) -> Image.Image:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì¤‘ê°„ í”„ë ˆì„ ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            PIL Image: ì¤‘ê°„ í”„ë ˆì„ ì´ë¯¸ì§€
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
        
        try:
            # ì´ í”„ë ˆì„ ìˆ˜ í™•ì¸
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames == 0:
                raise ValueError(f"Video has no frames: {video_path}")
            
            # ì¤‘ê°„ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
            middle_frame_idx = total_frames // 2
            cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_idx)
            
            # í”„ë ˆì„ ì½ê¸°
            ret, frame = cap.read()
            
            if not ret:
                raise ValueError(f"Failed to read middle frame from: {video_path}")
            
            # OpenCV (BGR) -> PIL (RGB) ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            logging.info(f"Extracted middle frame ({middle_frame_idx}/{total_frames}) from {video_path}")
            return pil_image
            
        finally:
            cap.release()
    
    def caption_with_blip2(self, image: Image.Image) -> str:
        """
        BLIP-2 ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
        
        Args:
            image: PIL Image
            
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        if self.processor is None or self.model is None:
            return "BLIP-2 model not initialized"
        
        try:
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
            if image.size[0] > 512 or image.size[1] > 512:
                image.thumbnail((512, 512), Image.Resampling.LANCZOS)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            inputs = self.processor(image, return_tensors="pt")
            
            # GPUë¡œ ì´ë™
            if "cuda" in self.device:
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                if torch.cuda.is_available():
                    inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}
            
            # ìº¡ì…˜ ìƒì„±
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, 
                    max_length=50, 
                    num_beams=5,
                    early_stopping=True,
                    no_repeat_ngram_size=2
                )
            
            # ë””ì½”ë”©
            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
            
            logging.info(f"BLIP-2 generated caption: {caption}")
            return caption
            
        except Exception as e:
            logging.error(f"BLIP-2 captioning failed: {e}")
            return f"Failed to generate caption: {str(e)}"
    
    def caption_with_gpt4v(self, image: Image.Image, api_key: str) -> str:
        """
        GPT-4V APIë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
        
        Args:
            image: PIL Image
            api_key: OpenAI API í‚¤
            
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp_file:
                # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (API ë¹„ìš© ì ˆì•½)
                if image.size[0] > 1024 or image.size[1] > 1024:
                    image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)
                
                image.save(tmp_file.name, "JPEG", quality=85)
                
                with open(tmp_file.name, "rb") as img_file:
                    base64_image = base64.b64encode(img_file.read()).decode('utf-8')
                
                os.unlink(tmp_file.name)
            
            # API ìš”ì²­
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            payload = {
                "model": "gpt-4-vision-preview",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Based on this video frame, imagine what scenes might exist to the left and right. "
            "Do not focus on describing specific objects or people. Instead, emphasize the overall atmosphere, "
            "lighting, color palette, and cinematic style of the surrounding environment. "
            "Describe it like a film director setting the tone for a wide, immersive shot. "
            "Keep the description under 100 words."
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "low"  # ë¹„ìš© ì ˆì•½
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 150,
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                caption = response.json()["choices"][0]["message"]["content"]
                logging.info(f"GPT-4V generated caption: {caption}")
                return caption
            else:
                error_msg = response.json().get("error", {}).get("message", "Unknown error")
                logging.error(f"GPT-4V API error: {response.status_code} - {error_msg}")
                return f"GPT-4V API error: {error_msg}"
                
        except Exception as e:
            logging.error(f"GPT-4V captioning failed: {e}")
            return f"Failed to generate caption with GPT-4V: {str(e)}"
    
    def caption_video(self, video_path: str, api_key: Optional[str] = None) -> str:
        """
        ë¹„ë””ì˜¤ ìº¡ì…”ë‹ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            api_key: API í‚¤ (API ë°©ì‹ ì‚¬ìš©ì‹œ)
            
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        try:
            # 1. ì¤‘ê°„ í”„ë ˆì„ ì¶”ì¶œ
            middle_frame = self.extract_middle_frame(video_path)
            
            # 2. ì„ íƒëœ ë°©ë²•ìœ¼ë¡œ ìº¡ì…”ë‹
            if self.method == "blip2":
                caption = self.caption_with_blip2(middle_frame)
            elif self.method == "gpt4v":
                if not api_key:
                    raise ValueError("API key required for GPT-4V method")
                caption = self.caption_with_gpt4v(middle_frame, api_key)
            else:
                raise ValueError(f"Unsupported captioning method: {self.method}")
            
            # 3. í›„ì²˜ë¦¬
            caption = self._post_process_caption(caption)
            
            return caption
            
        except Exception as e:
            logging.error(f"Video captioning failed: {e}")
            return f"Error: {str(e)}"
    
    def _post_process_caption(self, caption: str) -> str:
        """
        ìº¡ì…˜ í›„ì²˜ë¦¬ (ì •ì œ, í˜•ì‹ í†µì¼ ë“±)
        
        Args:
            caption: ì›ë³¸ ìº¡ì…˜
            
        Returns:
            str: í›„ì²˜ë¦¬ëœ ìº¡ì…˜
        """
        if not caption or caption.startswith(("Error:", "Failed", "GPT-4V API error:")):
            return caption
        
        # ê¸°ë³¸ì ì¸ ì •ì œ
        caption = caption.strip()
        
        # ì²« ê¸€ì ëŒ€ë¬¸ìí™”
        if caption and not caption[0].isupper():
            caption = caption[0].upper() + caption[1:]
        
        # ë§ˆì¹¨í‘œ ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
        if caption and not caption.endswith(('.', '!', '?')):
            caption += '.'
        
        return caption
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "method": self.method,
            "device": self.device,
            "model_loaded": self.model is not None,
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory": torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 0
        }

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def check_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸"""
    requirements = {
        "cv2": "opencv-python",
        "PIL": "Pillow", 
        "torch": "torch",
        "transformers": "transformers",
        "requests": "requests"
    }
    
    missing = []
    for module, package in requirements.items():
        try:
            if module == "cv2":
                import cv2
            elif module == "PIL":
                from PIL import Image
            else:
                __import__(module)
            print(f"âœ… {package} - OK")
        except ImportError:
            print(f"âŒ {package} - Missing")
            missing.append(package)
    
    if missing:
        print(f"\nğŸ”§ Install missing packages:")
        print(f"pip install {' '.join(missing)}")
        return False
    else:
        print("\nğŸ‰ All requirements satisfied!")
        return True

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_captioning(video_path: str, method: str = "blip2"):
    """ìº¡ì…”ë‹ í…ŒìŠ¤íŠ¸"""
    if not os.path.exists(video_path):
        print(f"âŒ Video file not found: {video_path}")
        return
    
    try:
        captioner = VideoCaptioning(method=method)
        print(f"ğŸ¬ Testing {method} captioning...")
        
        caption = captioner.caption_video(video_path)
        print(f"ğŸ“ Generated caption: {caption}")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        info = captioner.get_model_info()
        print(f"ğŸ”§ Model info: {info}")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")

if __name__ == "__main__":
    # ìš”êµ¬ì‚¬í•­ ì²´í¬
    print("ğŸ” Checking requirements...")
    if check_requirements():
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œ ë¹„ë””ì˜¤ ê²½ë¡œë¡œ ë³€ê²½)
        test_video = "/data/VACE/assets/videos/test.mp4"
        if os.path.exists(test_video):
            test_captioning(test_video)
        else:
            print(f"ğŸ“ Test video not found: {test_video}")
            print("ğŸ’¡ Place a test video and run: python video_captioning.py")